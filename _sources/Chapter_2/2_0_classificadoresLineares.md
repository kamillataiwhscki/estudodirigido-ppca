<style>
    main {
        text-align: justify;
    }
    legend {
        font-size: 16px;
    }
</style>

# CapÃ­tulo 2: Classificadores lineares

Os classificadores lineares tÃªm como princÃ­pio a separaÃ§Ã£o de classes por meio de funÃ§Ãµes discriminantes, que sÃ£o expressas como uma combinaÃ§Ã£o linear entre coeficientes e as caracterÃ­sticas dos padrÃµes. Esses classificadores se destacam por sua simplicidade e eficiÃªncia computacional. A definiÃ§Ã£o dos coeficientes, conhecidos como "pesos", varia de acordo com o mÃ©todo utilizado. No entanto, todos compartilham o mesmo objetivo: encontrar a melhor configuraÃ§Ã£o possÃ­vel para realizar a classificaÃ§Ã£o com precisÃ£o.

## Classificadores e problemas lineares

Pode-se definir um classificador como linear desde que o processo de discriminaÃ§Ã£o das classes abrangidas pelo problema seja conduzida atravÃ©s de $\textbf{superfÃ­cies de decisÃ£o lineares}$, ou seja, a decisÃ£o sobre a qual classe um dado pertence Ã© feita com base em uma combinaÃ§Ã£o linear de suas caracterÃ­sticas. 

Uma superfÃ­cie de decisÃ£o linear consiste no lugar geomÃ©trico que torna nula uma funÃ§Ã£o discriminante linear. Genericamente, uma funÃ§Ã£o linear Ã© dada por:

<div align=center>
    
$\begin{equation}
g(\textbf{x})=\textbf{w}^{T}\textbf{x} + ğœ”_{0} \tag{2.1}
\end{equation}$ </div>

em que $\textbf{x}$, $\textbf{w} \in \chi âŠ† \mathbb{R}^{n} $ e $ğœ”_{0} \in \mathbb{R}$. 

Cabe observar que a expansÃ£o de $\textbf{w}^{T}\textbf{x} + ğœ”_{0}$ gera uma combinaÃ§Ã£o linear, e ainda, $\textbf{w}^{T}\textbf{x}$ corresponde ao produto interno entre $\textbf{w}$ e $\textbf{x}^{1}$, sendo os dois organizados como vetores colunas, com $\textbf{w}$ transposto ($\textbf{w}^{T}$).

A fim de exibir determinadas caracterÃ­sticas e relaÃ§Ãµes relevantes a respeito das superfÃ­cies de decisÃ£o geradas por $g(\textbf{x})$, considere o espaÃ§o vetorial formado por vetores de duas dimensÃµes, $\mathbb{R}^{2}$, como o espaÃ§o de atributos. Por sua vez, com o intuito de reforÃ§ar o entendimento, a Figura 2.1 apoia esta discussÃ£o.

<div align="center">

![figura21](../images/lineares_figura22.png "figura 2.1") <legend>Figura 2.1 - CaracterÃ­sticas de uma superfÃ­cie de decisÃ£o linear.
</legend> </div>

A superfÃ­cie de decisÃ£o corresponde a um subconjunto de vetores $\textbf{x}$ no espaÃ§o de atributos que torna $g(\textbf{x})=0$. Ao tomar $\textbf{x}_{1}$ e $\textbf{x}_{2}$ sobre a superfÃ­cie de decisÃ£o, pode-se verificiar que:

<div align=center>
    
$\begin{equation}
\textbf{w}^{T}\textbf{x} + ğœ”_{0} = \textbf{w}^{T}\textbf{x}_{2} + ğœ”_{0} \Leftrightarrow \textbf{w}^{T}(\textbf{x}_{1}-\textbf{x}_{2})=0
\end{equation}$ </div>

levando a concluir que $\textbf{w}$ Ã© ortogonal Ã  superfÃ­cie de decisÃ£o linear, pois $\textbf{x}_{1}-\textbf{x}_{2}$ pode ser admitido como um vetor que determina a superfÃ­cie de decisÃ£o (neste caso uma reta) e a ortogonalidade mencionada decorre do produto interno nulo entre $\textbf{w}$ e o vetor $\textbf{x}_{1}-\textbf{x}_{2}$.

Considerando agora outros dois vetores $\textbf{x}_{3}=(x_{31},0)$ e $\textbf{x}_{4}=(0,x_{42})$ que tambÃ©m ocupam a superfÃ­cie de decisÃ£o, salvo detalhe que $\textbf{x}_{3}$ e $\textbf{x}_{4}$ interceptam o primeiro e segundo eixo do espaÃ§o de atributos, respectivamente. Com isso:

<div align=center>
    
$\begin{equation}
g(\textbf{x}_{3})=\textbf{w}^{T}\textbf{x}_{3} + ğœ”_{0} = 0 \Rightarrow ğœ”_{1}x_{31} + ğœ”_{2}0 + ğœ”_{0} = x_{31} = -\frac{ğœ”_{0}}{ğœ”_{1}}
\end{equation}$ </div>

<div align=center>

$\begin{equation}
g(\textbf{x}_{3})=\textbf{w}^{T}\textbf{x}_{4} + ğœ”_{0} = 0 \Rightarrow ğœ”_{1}0 + ğœ”_{2}x_{42} + ğœ”_{0} = x_{42} = -\frac{ğœ”_{0}}{ğœ”_{2}}
\end{equation}$ </div>

Esse resultado permite concluir que a distÃ¢ncia entre a superfÃ­cie de decisÃ£o e a origem do espaÃ§o de atributos equivale a $\frac{|ğœ”_{0}|}{||\textbf{w}||}$. Ainda, com base no mesmo conceito de distÃ¢ncia entre ponto e reta, Ã© possÃ­vel concluir que o mÃ³dulo de $g(\textbf{x})$ expressa a distÃ¢ncia entre $\textbf{x}$ e a superfÃ­cie de decisÃ£o.

AlÃ©m da noÃ§Ã£o de distÃ¢ncia entre os padrÃµes/vetores e a superfÃ­cie de decisÃ£o, Ã© de extrema importÃ¢ncia o valor do retorno gerado pela funÃ§Ã£o discriminante. Ao tomar um $\textbf{x}$ qualquer, tal que $g(\textbf{x}) > 0$, pode-se concluir que tal vetor estÃ¡ afastado da superfÃ­cie $g(\textbf{x})=0$ no mesmo sentido do vetor $\textbf{w}$. De forma similar, $g(\textbf{x}) < 0$ indica que o vetor ortogonal Ã  superfÃ­cie de decisÃ£o com extremidade em $\textbf{x}$ possui sentido oposto a $\textbf{w}$. Esta observaÃ§Ã£o a respeito do sinal de $g(\textbf{x})$ caracteriza a regra de decisÃ£o de um classificador linear, usualmente expressa por:

<div align=center>

$\begin{equation}
g(\textbf{x})=\textbf{w}^{T}\textbf{x} + ğœ”_{0}  \left \{ \begin{matrix} >0 â‡’ \textbf{x} \in ğœ”_{1} \\ < 0 â‡’ \textbf{x} \in ğœ”_{2} \end{matrix} \right. \tag{2.2}
\end{equation}$ </div>

De acordo com a equaÃ§Ã£o acima, verifica-se que um classificador linear possibilita a distinÃ§Ã£o entre duas classes somente.

Antes de prosseguir os estudos sobre os classificadores lineares, Ã© importante que se saiba a diferenÃ§a entre $\textit{classificadores lineares}$ e $\textit{problemas linearmente separÃ¡veis}$. Conforme jÃ¡ definido, os classificadores lineares sÃ£o estruturados segundo funÃ§Ãµes discriminantes com forma equivalente Ã  apresentada na EquaÃ§Ã£o 2.1. Por outro lado, um problema de classificaÃ§Ã£o Ã© considerado linearmente separÃ¡vel desde que seja possÃ­vel obter, pelo menos, uma superfÃ­cie de decisÃ£o linear que distinguia dois tipos de objetos cujas classes sejam conhecidas de antemÃ£o (Figura 2.2). Isso permite concluir que a capacidade de separaÃ§Ã£o atravÃ©s de uma superfÃ­cie linear Ã© uma propriedade dos dados a serem classificados e nÃ£o do classificador em si.

<div align="center"> 

![figura21](../images/figura22-linearmenteseparavel.png "figura 2.2") <legend>Figura 2.2 - Exemplos de casos de separabilidade. Para dados linearmente separÃ¡veis, Ã© garantida a existÃªncia de uma superfÃ­cie linear capaz de particionar o espaÃ§o de atributos em dois subconjuntos de classes distintas, sendo esta afirmaÃ§Ã£o nÃ£o garantida para o caso nÃ£o linearmente separÃ¡vel.
</legend> </div>
